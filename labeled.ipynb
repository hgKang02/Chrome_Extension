{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96cd6fe-cfa0-4233-a5e1-2f74e0a34d8a",
   "metadata": {},
   "source": [
    "### Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263d0559-d81c-42a6-b84f-eb9da163d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pytube\n",
    "!pip install librosa\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install IPython\n",
    "!pip install numba\n",
    "!pip install ffmpeg\n",
    "!pip install torch\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bfe38-2103-4667-8a3a-81a1a34fdaad",
   "metadata": {},
   "source": [
    "### PyTube (downloading youtube video from link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbb1123-2e5b-4234-95c4-e0781b129f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing packages\n",
    "# from pytube import YouTube\n",
    "# import os\n",
    "# # url input from user\n",
    "# yt = YouTube(str(input(\"Enter the URL of the video you want to download: \\n>> \")))\n",
    "# # extract only audio\n",
    "# video = yt.streams.filter(only_audio=True).first()\n",
    "# # check for destination to save file\n",
    "# print(\"Enter the destination (leave blank for current directory)\")\n",
    "# destination = str(input(\">> \")) or '.'\n",
    "# # download the file\n",
    "# out_file = video.download(output_path=destination)\n",
    "# # save the file\n",
    "# base, ext = os.path.splitext(out_file)\n",
    "# print(\"Enter file type (mp3 or wav)\")\n",
    "# file_type = str(input(\">> \"))\n",
    "# new_file = base + '.' + file_type\n",
    "# os.rename(out_file, new_file)\n",
    "# # result of success\n",
    "# print(yt.title + \" has been successfully downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99eb13d-8a4e-46be-a6a0-9e7402ec8e3e",
   "metadata": {},
   "source": [
    "### Speech-to-Text using wav2vec 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a990d908-4513-4eca-9216-f6a4fd11eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:736: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-robust-ft-libri-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-robust-ft-libri-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 44100 Hz\n",
      "IN TO DAY'S VIDIO I'M SHOWING YOU HOW YOU CAN MAKE A TEXA SPEECH VIDIO JEST LIKE THI EECENTLY WENT TO IRELAND  IT'S A BEAUTIFUL COUNTRYIN ONE MINUT ND THIS EC SAMPO OR ONI USE SPEECH ONLINE VIDIO ADTED TER AND TO GETCH STARTED ILICKCON O FERTS LINK IN  THESCRICTION NOW WE AUTOMATICALLY STARTED THE NEW PROJECT AND I WANT IMPORT MY VDIO TO MAKE THIS HAPPEN I'M GONG TO CLICK ON UPLOT O FILE SELECT A VIDIO AND CLICK ON OPEN AFTER A MOMENT THE VIDIOIS IMPORTED AND WEUH CAN GET STARTE TO CREAT E TEXT A SPEECH FRAKMENT HD OVER TO AUT IO N THE LEFT MENU AND CLICK ON TEXAS SPEECH FROM HERE YOU CAN SELECT A SP CIFIC LANGUAGE A VOICE AND A TXTT TYOU WOULD LAEKT E TURN INTO SPEECH TOLITOME BACK TO IT CLIK OM PRE VIEW I RECENTLY WENT TO IRELAND THEN IF YUR SA ISFIED YOU CAN CLIK ON AT TO PROJECT TO ADD ET TO THE PROTJXT EQUEDS FROM HEAA YOU CAN A JUST THE AUTIO SADINGS IN THE ADD ET AUTDIO MENU AND FURTHER ADJUST THE TIMING OF THE SPEECH FRAGMENT IN THE BOT IM TI'ME LY' MENU YOU CAN RE PEAT THE PROCEST AND ADD AS MANY SPEECH TO TEX FRAGMENT WITH DIFFREN LANGUAGES VOICES AND SEND TIN TES AS YOU LIKE ADDITIONALLY YOU CAN EVEN TUR THE TECT A SPEEH FRAGMENTS INTO CUSTOMIZABLE SUP TITLES TO ADD MORE CONTECT AND FLAR TO YOUR SPEEACH TO TEXT VIDIOS AND THEN WHEN YOUR ST ISFIED YOU CAN EASILY EXPORT YOUR VIDIO WOR TE EXTPORT BUT  AND THE EXPORT MAN U AND AFTER IF VIDO'S RANET YOUARE ABLE TO INST ANTLY  SHARE THE VIDIO OR YOU CAN DOWN OT TET AS AN AND BEFORE STRAIGHT TIYOUR DEVICE AND THAT'S IT OR THIS VDIO TINK E SOMEMUCH FOR WATCHING AND GOOD LUCK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-robust-ft-libri-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-robust-ft-libri-960h\")\n",
    "\n",
    "file_name = '1speech.wav'\n",
    "\n",
    "data = wavfile.read(file_name)\n",
    "framerate = data[0]\n",
    "sounddata = data[1]\n",
    "time = np.arange(0,len(sounddata))/framerate\n",
    "print('Sampling rate:',framerate,'Hz')\n",
    "\n",
    "input_audio, rate = librosa.load(file_name, sr=16000)\n",
    "\n",
    "input_values = tokenizer(input_audio, return_tensors=\"pt\").input_values\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32040677-5998-4626-9d81-c6aed11cc9bc",
   "metadata": {},
   "source": [
    "### PEGASUS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd15a8c9-6791-456a-809e-709490dca7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images are copyrighted.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").to(device)\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "text = transcription\n",
    "\n",
    "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "# summary = model.generate(**tokens)\n",
    "\n",
    "# tokenizer.batch_decode(summary, skip_special_tokens=True)\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=32, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
